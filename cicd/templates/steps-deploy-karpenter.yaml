steps:
  # Install Helm
  - task: HelmInstaller@0
    displayName: 'Install Helm'
    inputs:
      helmVersion: '3.x'

    # Update aws-auth ConfigMap
  - task: AWSShellScript@1
    displayName: 'Update aws-auth ConfigMap'
    inputs:
      awsCredentials: '$(awsConnection)'
      regionName: '$(region)'
      scriptType: 'inline'
      inlineScript: |
        set -ex
        # Set environment variables
        AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
        AWS_PARTITION="aws"  # Change this if you are using a different partition like aws-cn or aws-us-gov
        CLUSTER_NAME=$(clusterName)

        # Update kubeconfig
        aws eks --region $(region) update-kubeconfig --name $(clusterName)

        # Retrieve the current aws-auth ConfigMap
        kubectl get configmap aws-auth -n kube-system -o yaml > aws-auth.yaml

        # Add the new role to the mapRoles section
        yq eval '.data.mapRoles += "\n- rolearn: arn:'${AWS_PARTITION}':iam::'${AWS_ACCOUNT_ID}':role/KarpenterNodeRole-'${CLUSTER_NAME}'\n  username: system:node:{{EC2PrivateDNSName}}\n  groups:\n  - system:bootstrappers\n  - system:nodes"' -i aws-auth.yaml

        # Apply the updated ConfigMap
        kubectl apply -f aws-auth.yaml

  # Deploy Karpenter via Helm
  - task: AWSShellScript@1
    displayName: 'Deploy Karpenter'
    inputs:
      awsCredentials: '$(awsConnection)'
      regionName: '$(region)'
      scriptType: 'inline'
      inlineScript: |
        set -ex
        AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
        AWS_DEFAULT_REGION=$(region)
        KARPENTER_NAMESPACE="karpenter"
        CLUSTER_NAME=$(clusterName)
        

        # Set Karpenter version
        export KARPENTER_VERSION="1.0.8"

        # Update kubeconfig
        aws eks --region $(region) update-kubeconfig --name $(clusterName)
        
        # Get the node group name
        NODEGROUP=$(aws eks list-nodegroups --cluster-name $CLUSTER_NAME --query 'nodegroups[0]' --output text)
        # Export the NODEGROUP environment variable
        export NODEGROUP
        
        # Add Karpenter Helm repository
        helm repo add karpenter https://charts.karpenter.sh/
        helm repo update

        # Generate Karpenter deployment YAML
        helm template karpenter oci://public.ecr.aws/karpenter/karpenter --version "${KARPENTER_VERSION}" --namespace "${KARPENTER_NAMESPACE}" \
            --set "settings.clusterName=${CLUSTER_NAME}" \
            --set "settings.interruptionQueue=${CLUSTER_NAME}" \
            --set "serviceAccount.annotations.eks\.amazonaws\.com/role-arn=arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterControllerRole-${CLUSTER_NAME}" \
            --set controller.resources.requests.cpu=1 \
            --set controller.resources.requests.memory=1Gi \
            --set controller.resources.limits.cpu=1 \
            --set controller.resources.limits.memory=1Gi > karpenter.yaml

        # Modify node affinity in karpenter.yaml
        sed -i '/affinity:/a\
          nodeAffinity:\
            requiredDuringSchedulingIgnoredDuringExecution:\
              nodeSelectorTerms:\
              - matchExpressions:\
                - key: karpenter.sh/nodepool\
                  operator: DoesNotExist\
                - key: eks.amazonaws.com/nodegroup\
                  operator: In\
                  values:\
                  - ${NODEGROUP}\
          podAntiAffinity:\
            requiredDuringSchedulingIgnoredDuringExecution:\
              - topologyKey: "kubernetes.io/hostname"' karpenter.yaml

        # Create Karpenter namespace and apply the YAML
        kubectl create namespace "${KARPENTER_NAMESPACE}" || true
        # kubectl create -f "https://raw.githubusercontent.com/aws/karpenter-provider-aws/v${KARPENTER_VERSION}/pkg/apis/crds/karpenter.sh_nodepools.yaml"
        # kubectl create -f "https://raw.githubusercontent.com/aws/karpenter-provider-aws/v${KARPENTER_VERSION}/pkg/apis/crds/karpenter.k8s.aws_ec2nodeclasses.yaml"
        kubectl create -f "https://raw.githubusercontent.com/aws/karpenter-provider-aws/v${KARPENTER_VERSION}/pkg/apis/crds/karpenter.sh_nodeclaims.yaml"
        kubectl apply -f karpenter.yaml

        # Create default NodePool
        cat <<EOF | envsubst | kubectl apply -f -
        apiVersion: karpenter.sh/v1
        kind: NodePool
        metadata:
          name: default
        spec:
          template:
            spec:
              requirements:
                - key: kubernetes.io/arch
                  operator: In
                  values: ["amd64"]
                - key: kubernetes.io/os
                  operator: In
                  values: ["linux"]
                - key: karpenter.sh/capacity-type
                  operator: In
                  values: ["spot"]
                - key: karpenter.k8s.aws/instance-category
                  operator: In
                  values: ["c", "m", "r"]
                - key: karpenter.k8s.aws/instance-generation
                  operator: Gt
                  values: ["2"]
              nodeClassRef:
                group: karpenter.k8s.aws
                kind: EC2NodeClass
                name: default
              expireAfter: 720h # 30 * 24h = 720h
          limits:
            cpu: 1000
          disruption:
            consolidationPolicy: WhenEmptyOrUnderutilized
            consolidateAfter: 1m
        ---
        apiVersion: karpenter.k8s.aws/v1
        kind: EC2NodeClass
        metadata:
          name: default
        spec:
          amiFamily: AL2 # Amazon Linux 2
          role: "KarpenterNodeRole-${CLUSTER_NAME}" # replace with your cluster name
          subnetSelectorTerms:
            - tags:
                karpenter.sh/discovery: "${CLUSTER_NAME}" # replace with your cluster name
          securityGroupSelectorTerms:
            - tags:
                karpenter.sh/discovery: "${CLUSTER_NAME}" # replace with your cluster name
          amiSelectorTerms:
            - id: "ami-0a7a03417a876ba05"  # ARM AMI ID
            - id: "ami-0c76bd4bd302b30ec"  # AMD AMI ID
        #   - id: "${GPU_AMI_ID}" # <- GPU Optimized AMD AMI 
        #   - name: "amazon-eks-node-${K8S_VERSION}-*" # <- automatically upgrade when a new AL2 EKS Optimized AMI is released. This is unsafe for production workloads. Validate AMIs in lower environments before deploying them to production.
        EOF

  # # Install Karpenter via Helm
  # - task: AWSShellScript@1
  #   displayName: 'Deploy Karpenter'
  #   inputs:
  #     awsCredentials: '$(awsConnection)'
  #     regionName: '$(region)'
  #     scriptType: 'inline'
  #     inlineScript: |
  #       set -ex
  #       AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
  #       AWS_DEFAULT_REGION=$(region)

  #       # Update kubeconfig
  #       aws eks --region $(region) update-kubeconfig --name $(clusterName)
        
  #       # List all Helm releases in all namespaces
  #       helm list --all-namespaces

  #       helm repo add karpenter https://charts.karpenter.sh/
  #       helm repo update
        
  #       helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter \
  #         --version v0.32.1 \
  #         --namespace karpenter \
  #         --create-namespace \
  #         --set "settings.aws.clusterName=$(clusterName)" \
  #         --set "settings.aws.defaultInstanceProfile=KarpenterNodeInstanceProfile-$(clusterName)" \
  #         --set "serviceAccount.annotations.eks\.amazonaws\.com/role-arn=arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterControllerRole-$(clusterName)"

  # Create Karpenter Provisioner
  - task: AWSShellScript@1
    displayName: 'Configure Karpenter Provisioner'
    inputs:
      awsCredentials: '$(awsConnection)'
      regionName: '$(region)'
      scriptType: 'inline'
      inlineScript: |
        set -ex
        cat <<EOF | kubectl apply -f -
        apiVersion: karpenter.sh/v1beta1
        kind: Provisioner
        metadata:
          name: default
        spec:
          requirements:
            - key: karpenter.k8s.aws/instance-category
              operator: In
              values: ["c", "m", "r"]
            - key: karpenter.k8s.aws/instance-generation
              operator: In
              values: ["5", "6"]
          limits:
            resources:
              cpu: 1000
          providerRef:
            name: default
        ---
        apiVersion: karpenter.k8s.aws/v1beta1
        kind: AWSNodeTemplate
        metadata:
          name: default
        spec:
          subnetSelector:
            karpenter.sh/discovery: "$(clusterName)"
          securityGroupSelector:
            karpenter.sh/discovery: "$(clusterName)"
        EOF

  ## deploy karpenter node role
  # - task: AWSShellScript@1
  #   displayName: 'KarpenterNodeRole'
  #   inputs:
  #     awsCredentials: '$(awsConnection)'
  #     regionName: '$(region)'
  #     scriptType: 'inline'
  #     inlineScript: |
  #       set -ex
  #       # Update kubeconfig
  #       aws eks --region $(region) update-kubeconfig --name $(clusterName)
        
  #       AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
  #       KARPENTER_IAM_ROLE_NAME="KarpenterNodeRole-$(clusterName)"
        
  #       # Create IAM role and instance profile
  #       aws iam create-role \
  #         --role-name "${KARPENTER_IAM_ROLE_NAME}" \
  #         --assume-role-policy-document file://$(System.DefaultWorkingDirectory)/cicd/templates/karpenter-node-role-trust-policy.json
        
  #       aws iam attach-role-policy \
  #         --role-name "${KARPENTER_IAM_ROLE_NAME}" \
  #         --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        
  #       aws iam attach-role-policy \
  #         --role-name "${KARPENTER_IAM_ROLE_NAME}" \
  #         --policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        
  #       aws iam attach-role-policy \
  #         --role-name "${KARPENTER_IAM_ROLE_NAME}" \
  #         --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        
  #       aws iam attach-role-policy \
  #         --role-name "${KARPENTER_IAM_ROLE_NAME}" \
  #         --policy-arn arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore

##deploying karpenter controller role
  # - task: AWSShellScript@1
  #   displayName: 'KarpenterControllerRole'
  #   inputs:
  #     awsCredentials: '$(awsConnection)'
  #     regionName: '$(region)'
  #     scriptType: 'inline'
  #     inlineScript: |
  #       set -ex
  #       # Update kubeconfig
  #       aws eks --region $(region) update-kubeconfig --name $(clusterName)

  #       AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
  #       AWS_PARTITION=$(aws sts get-caller-identity --query Arn --output text | cut -d":" -f2)
  #       OIDC_ENDPOINT=$(aws eks describe-cluster --name $(clusterName) --query "cluster.identity.oidc.issuer" --output text)
  #       OIDC_PROVIDER=$(aws eks describe-cluster --name eks-karpenter-eks --query "cluster.identity.oidc.issuer" --output text | sed -e "s/^https:\/\///")

  #       # Define the trust policy document for the Karpenter Controller role
  #       KARPENTER_CONTROLLER_TRUST_POLICY=$(cat <<EOF
  #       {
  #         "Version": "2012-10-17",
  #         "Statement": [
  #           {
  #             "Effect": "Allow",
  #             "Principal": {
  #               "Federated": "arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}"
  #             },
  #             "Action": "sts:AssumeRoleWithWebIdentity",
  #             "Condition": {
  #               "StringEquals": {
  #                 "${OIDC_PROVIDER}:sub": "system:serviceaccount:karpenter:karpenter"
  #               }
  #             }
  #           }
  #         ]
  #       }
  #       EOF
  #       )

  #       # Create the Karpenter Controller role with the trust policy
  #       aws iam create-role --role-name "KarpenterControllerRole-$(clusterName)" \
  #         --assume-role-policy-document "${KARPENTER_CONTROLLER_TRUST_POLICY}"

  #       # Define the inline policy document for Karpenter Controller role
  #       KARPENTER_CONTROLLER_POLICY=$(cat <<EOF
  #       {
  #         "Version": "2012-10-17",
  #         "Statement": [
  #           {
  #             "Effect": "Allow",
  #             "Action": [
  #               "eks:DescribeCluster",
  #               "sts:AssumeRole"
  #             ],
  #             "Resource": "*"
  #           },
  #           {
  #             "Effect": "Allow",
  #             "Action": [
  #               "ec2:DescribeInstances",
  #               "ec2:DescribeInstanceTypes",
  #               "ec2:DescribeSecurityGroups",
  #               "ec2:DescribeKeyPairs",
  #               "ec2:DescribeSubnets",
  #               "ec2:DescribeRouteTables",
  #               "ec2:DescribeVpcs",
  #               "ec2:DescribeAvailabilityZones",
  #               "ec2:DescribeImages"
  #             ],
  #             "Resource": "*"
  #           }
  #         ]
  #       }
  #       EOF
  #       )

  #       # Attach the inline policy to the Karpenter Controller role
  #       aws iam put-role-policy --role-name "KarpenterControllerRole-$(clusterName)" \
  #         --policy-name "KarpenterControllerPolicy" \
  #         --policy-document "${KARPENTER_CONTROLLER_POLICY}"

  ## Install Karpenter CRDs
  # - task: AWSShellScript@1
  #   displayName: 'Install Karpenter CRDs'
  #   inputs:
  #     awsCredentials: '$(awsConnection)'
  #     regionName: '$(region)'
  #     scriptType: 'inline'
  #     inlineScript: |
  #       set -ex
  #       # Update kubeconfig
  #       aws eks --region $(region) update-kubeconfig --name $(clusterName)

  #       # Install Karpenter CRDs
  #       kubectl apply -f https://raw.githubusercontent.com/aws/karpenter/main/pkg/apis/crds/karpenter.sh_provisioners.yaml
  #       kubectl apply -f https://raw.githubusercontent.com/aws/karpenter/main/pkg/apis/crds/karpenter.sh_awsnodetemplates.yaml
